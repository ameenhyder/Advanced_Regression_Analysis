{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413be14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccc2e7",
   "metadata": {},
   "source": [
    "1. Create a function to read the CSV file provided into a DataFrame. \n",
    "2. You MUST place the CSV file in the same directory/folder where your notebook is located. The method below should work without change when you give the file name \"stroke-data.csv\". \n",
    "4. The file imported has NA (not available) values in some columns. These rows need to be dropped as machine learning algorithms cannot process data with missing values. Remember when rows are dropped some (row) indexes will be missing. \n",
    "3. The first step in processing data is to review the data types of the features (columns). \n",
    "4. Use **pandas** features *columns* and *dtypes* to create a dictionary with column names as keys and the datatype as values.\n",
    "5. This function then returns the new dataframe (df) and the df_types dictionary (df_types), where a key-value pair represents column name-column's dtype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9c197120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': dtype('int64'),\n",
       " 'gender': dtype('O'),\n",
       " 'age': dtype('float64'),\n",
       " 'hypertension': dtype('int64'),\n",
       " 'heart_disease': dtype('int64'),\n",
       " 'ever_married': dtype('O'),\n",
       " 'avg_glucose_level': dtype('float64'),\n",
       " 'bmi': dtype('float64'),\n",
       " 'smoking_status': dtype('O'),\n",
       " 'stroke': dtype('int64')}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data(fl):\n",
    "    \n",
    "    # Import the CSV file (fl)\n",
    "    # Your code goes here\n",
    "    df2 = pd.read_csv(fl)\n",
    "        \n",
    "    # Drop all rows with NA values\n",
    "    # Your code goes here\n",
    "    df2 = df2.dropna()\n",
    "    \n",
    "    # Create a dictionary with keys the column names and values the type of data\n",
    "    # Your code goes here\n",
    " \n",
    "    df2_types = {}\n",
    "    \n",
    "    for i in range(len(df2.columns)):\n",
    "        df2_types[df2.columns[i]] = df2.dtypes[i]\n",
    "\n",
    "    return df2, df2_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90968fc6",
   "metadata": {},
   "source": [
    "Many machine learning algorithms are designed to process numeric data and cannot natively handle categorical data. Therefore as part of the model building process, we must apply pre-processing steps to convert the data into an encoded format which the algorithms can handle.\n",
    "\n",
    "1. In the following function you will identify and convert categorical variables to numeric data type. \n",
    "2. You will need the python *dictionary* \"df2_types\" of the function \"process_data\" we created in task 1. We can use this to identify data in a categorical (non-numeric) data format.\n",
    "3. Create a list \"cat_ls\" of column names which are non-numeric. \n",
    "4. Process each column named in \"cat_ls\" separately. \n",
    "5. For a column name, say \"col_name\", find the *distinct* categories. For example, in column \"gender\" there are 2 categories \"Male\" and \"Female\". \n",
    "6. For a (categorical) column 'C' with *k* categories *k-1* new columns are created and 'C' is replaced by these new columns. For example, the \"*gender*\" column will be replaced by one numerical column. The column \"*smoking_status*\" is to be replaced with 2 numerical columns. This process is referred to as *one-hot encoding*.\n",
    "7. The encoding is done as follows. Suppose there are 3 categories \"cat1\", \"cat2\", \"cat3\" in column 'C'. Create 2 columns with distinct names, say \"cat_level1\", \"cat_level2. If an observation corresponding to a row is 'cat1' then put a 1 in 'cat_level1' and 0 in 'cat_level2' in the same row. If it is 'cat2' put 0 in 'cat_level1' and 1 in 'cat_level2' and put 0 in both if the observation is 'cat3'. \n",
    "8. It is simpler if the column has only 2 categories (like \"gender\"). It will be replaced by 1 column of 1's and 0's. \n",
    "9. The number of columns in the new DataFrame will be generally more than the original. For the *stroke-dataset* this number is 11. Remember to **drop** the old non-numeric columns.  \n",
    "10. Depending on how you do it the column orderings may change. This is important for identifying the output column \"stroke\". \n",
    "11. You may reorder the columns. Suggestion:move \"stroke\" to the last column in the new dataframe. \n",
    "13. You should NOT use any feature-processing modules from **sklearn** or pandas.get_dummies()for this part. If used the maximum mark for this task will not exceed 60%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "201addf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y,df,ColumnToEncode,column_name):\n",
    "    nb_classes = len(np.unique(y)) # get the number of unique classes\n",
    "    standardised_labels = dict(zip(np.unique(y), np.arange(nb_classes))) # get the class labels as a dictionary\n",
    "    targets = np.vectorize(standardised_labels.get)(y) # map the dictionary values to array.\n",
    "    df_t = pd.DataFrame(np.eye(nb_classes)[targets])\n",
    "    df_t.columns = \n",
    "    df_t =  df_t.iloc[:,0:(len(df_t.columns)-1)]\n",
    "    \n",
    "    df = pd.concat([df.reset_index(drop = True), # Cbind DataFrames\n",
    "                            df_t],\n",
    "                           axis = 1)\n",
    "    \n",
    "    df = df.drop([ColumnToEncode], axis=1)\n",
    "    return df\n",
    "\n",
    "def convert_to_numeric():\n",
    "    \n",
    "    # Read the appropriate file, should be in the same directory as the notebook\n",
    "    df, dict_types = process_data(\"Stroke_data.csv\")\n",
    "    df_temp = df    \n",
    "    \n",
    "    # Apply the one hot encoding process outlined to the new dataframe df2\n",
    "    # Your code goes here\n",
    "    df2 = None \n",
    "    ColumnsToEncode = list(df_temp.select_dtypes(include=['category','object']))\n",
    "    \n",
    "    for i in range(len(ColumnsToEncode)):\n",
    "        df_temp = one_hot_encode(df_temp[ColumnsToEncode[i]],df_temp,ColumnsToEncode[i],\n",
    "                                 list(df_temp[ColumnsToEncode[i]].unique()))\n",
    "    \n",
    "    df2 = df_temp\n",
    "    \n",
    "    return df2       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79220aa5",
   "metadata": {},
   "source": [
    "1. Convert all columns except \"id\" and \"stroke\" into a numerical feature matrix **X**. The size of the matrix will be *no_of_rows* $\\times$  *(no_of_columns-2)*. The number of columns should be 9. \n",
    "2. Put the values in the \"stroke\" column in the array **y**. \n",
    "3. Use the sklearn [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to generate *X_train, X_test, y_train. y_test*. \n",
    "5. In the *train_test_split()* method the fraction of data to be split for testing has to be specified. Vary this fraction between .2 to .33. Run your program  a few times to choose  an optimim value. The optimum will correspond to the fraction giving the best accuracy/precision (see Task 5). \n",
    "6. Return the 4 arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "47e157e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_arrays(size):\n",
    "    \n",
    "    # Call the function created in Task 2 to source the encoded data frame\n",
    "    df = convert_to_numeric()\n",
    "    \n",
    "    # Create the X and y objects\n",
    "    # Your code goes here\n",
    "    X = np.array(df.drop(['id', 'stroke'], axis=1))\n",
    "    y = np.array(df['stroke'])\n",
    "    \n",
    "    # Create test/train splits for X and y\n",
    "    # Your code goes here\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = size)\n",
    "    \n",
    "    # Function returns the four newly created objects\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9abf4",
   "metadata": {},
   "source": [
    "1. In the following function you will use the [liner_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from sklearn to create and train a logistic regression model. \n",
    "2. The model should be trained on the train set created in task 3. **Do not use the full dataset or test set for training**.\n",
    "2. As this is a binary classification problem (2 classes: \"stroke\", \"no-stroke\") the default model does not need significant adjustment\n",
    "3. You should refer to the document and experiment with changing the hyperparameters of the model\n",
    "\n",
    "\n",
    "Once you have a trained a model, answer the below questions:\n",
    "1. In the LogisticRegression class, the first keyword argument is *penalty='l2'*. What is penalized and why? Explain this in 2 sentences.  \n",
    "Answer: the penalization means regularizing the linear or regression model to avoid overfitting and reduce the impact of some high magnitude coeffecients. In other words, it reduces the impact of parameters in the model and simplifies the model.\n",
    "2. Instead of $l_2$ penalty one may use $l_1$ penalty? What is the difference between the two?  \n",
    "L1 introduces the penalty equal to the absolute value of coefficient's model and may eliminate a feature completely from a model. Whereas, L2 introduces the penalty equal to the square of the magnitude of the coefficients. It doesnot eliminate a feature rather shrinks the impact of each every coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "36086afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07348576,  0.5010995 ,  0.44139574,  0.00351817, -0.00437526,\n",
       "         0.04879587,  0.15959432, -0.46904383, -0.42487511]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_logitmodel(X, y):\n",
    "    \n",
    "    # Create the logitmodel_stroke model\n",
    "    # Your code goes here\n",
    "    logitmodel_stroke = None\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    LogRegr_Mod = LogisticRegression()\n",
    "    \n",
    "    \n",
    "    # Train the logitmodel_stroke model\n",
    "    # Your code goes here\n",
    "    logitmodel_stroke = LogRegr_Mod.fit(X,y)\n",
    "    \n",
    "    return logitmodel_stroke\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_arrays(0.3)\n",
    "model = fit_logitmodel(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0cdb2",
   "metadata": {},
   "source": [
    "The process for evaluating a classification model is different from a regression model. In regression we have a wide range of values so we measure variance, however classification has a much smaller problem space so we measure how often the correct prediction is made. There are multiple metrics for measuring this, [this article](https://www.mage.ai/blog/definitive-guide-to-accuracy-precision-recall-for-product-developers) and the [Wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) provide additional context.\n",
    "\n",
    "1. As this is binary classification there are 2 classes. Class 1 indicates positive stroke risk and class 0 indicates negative stroke risk. \n",
    "2. When testing we use a separate dataset which the model was not trained on. This is essential to observe how the model performs on data it has not seen before.\n",
    "3. In the function below *X_ts* represents the data used to generate test predictions and *y_obs* represents the actual values we are trying to predict. \n",
    "4. We can evaluate a classification model by having it make a set of predictions for a test set (X_ts) and comparing these with the actual values (y_obs).\n",
    "5. Suppose *y_pred* is a predicted value when run on a sample from *X_ts*. We compare it to the corresponding observed value in *y_obs*. There are four potential outcomes from this comparison:\n",
    "\n",
    "    1. *y_pred* = 1 (positive) and *y_obs* = 1 (positive): counted as *true positive*.\n",
    "    2. *y_pred* = 1 (positive) and *y_obs* = 0 (negative): counted as *false positive*. \n",
    "    3. *y_pred* = 0 (negative) and *y_obs* = 0 (negative): counted as *true negative*. \n",
    "    4. *y_pred* = 0 (positive) and *y_obs* = 1 (negative): counted as *false negative*. \n",
    "    \n",
    "5. Count all the 4 cases for the entire sample input to the function *evaluate_logitmodel* and store them in 4 variables: *tp*, *fp*, *tn* and *fn*. For example, *tp* will give total number of true positives and *fn* the total of true negatives. \n",
    "6. The two metrics we will be using for evaluation are *accuracy* and *precision*. The formula for these is below. \n",
    "$$acc = \\frac{tp+tn}{tp+tn+fp+fn} \\quad\\text{(accuracy)}, \\quad prec = \\frac{tp}{tp + fp} \\quad\\text{(precision)}$$\n",
    "\n",
    "7. Run the model training/evaluation process for 5 different test/train split ratios (see task 3). Add a paragraph below outlining:\n",
    "    1. The results of your different test/train splits\n",
    "    2. How the different split sizes effected model evaluation\n",
    "    3. Was there a difference in accuracy/precision and if so, what could be causing this?\n",
    "    4. For a fixed train/test data evaluate the metrics on the train data (*X_train*) and test (*X_test*) seprately and record the valuse of the metrics.  \n",
    "8. This task is designed to test your understanding of model evaluation. **No built-in evaluation functions or metrics should be used**. \n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "as the train/test split ration increases with more data going into test split, the accuracy increases from 0.93 to 0.94. However, precision cannot be calculated as model is not predicting any stroke value as both false positive and true positive value are 0. This due to highly skewed data being present in the training set as there are only 180 values out of approx 3000 values that have stroke = 1. Yes there is difference between accuracy and precision, as accuracy takes into both true positives and true negatives. Its value is also close to 95percent because even if it predicts all test data points as negative, since data is highly skewed, it will be high, however, precision is not defined or close to zero as it is not prediciting any positive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "aed60c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model object is the output of the function fit_logitmodelto obtain y_pred\n",
    "def evaluate_logitmodel(model, X_ts,  y_obs):\n",
    "    \n",
    "    # Use the .predict() method of the model to generate a set of predictions for X_ts\n",
    "    # Your code goes here\n",
    "    \n",
    "    y_pred = model.predict(X_ts)\n",
    "    \n",
    "    # Determine the tp, fp, tn and fn values for the prediction set\n",
    "    # Your code goes here\n",
    "    tp = np.sum(np.logical_and(y_pred == 1, y_obs == 1))\n",
    "    tn = np.sum(np.logical_and(y_pred == 0, y_obs == 0))\n",
    "    fp = np.sum(np.logical_and(y_pred == 1, y_obs == 0))\n",
    "    fn = np.sum(np.logical_and(y_pred == 0, y_obs == 1))\n",
    "    \n",
    "    \n",
    "    # Calculate the accuracy and precision values\n",
    "    # Your code goes here\n",
    "    acc = (tp + tn)/(tp+tn+fp+fn)\n",
    "    prec = tp/(tp+fp)\n",
    "    \n",
    "    return acc,prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "49673bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurracy for iteration  0  is: 0.9357664233576642\n",
      "prec for iteration  0  is: nan\n",
      "accurracy for iteration  1  is: 0.9369894982497082\n",
      "prec for iteration  1  is: nan\n",
      "accurracy for iteration  2  is: 0.9489583333333333\n",
      "prec for iteration  2  is: 0.0\n",
      "accurracy for iteration  3  is: 0.9426070038910506\n",
      "prec for iteration  3  is: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ameen.hyder\\AppData\\Local\\Temp\\ipykernel_17956\\3936860572.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  prec = tp/(tp+fp)\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ameen.hyder\\AppData\\Local\\Temp\\ipykernel_17956\\3936860572.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  prec = tp/(tp+fp)\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "test_size = [0.2,0.25,0.28,0.3,0.33]\n",
    "\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = create_arrays(test_size[i])\n",
    "    model = fit_logitmodel(X_train, y_train)\n",
    "    \n",
    "    acc,prec = evaluate_logitmodel(model,X_test,y_test)\n",
    "    \n",
    "    print(\"accurracy for iteration \",i,\" is:\", acc)\n",
    "    print(\"prec for iteration \",i,\" is:\", prec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
